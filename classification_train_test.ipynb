{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Linear Classifier with Train/Test Split\n",
        "\n",
        "This notebook implements the random linear classifier algorithm with proper train/test data splitting.\n",
        "\n",
        "## Key Improvements:\n",
        "- Split data into training (80%) and testing (20%)\n",
        "- Train algorithm only on training data\n",
        "- Evaluate performance on unseen test data\n",
        "- Better assessment of generalization capability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for machine learning with train/test split\n",
        "import numpy as np                                    # For mathematical operations\n",
        "import matplotlib.pyplot as plt                       # For creating visualizations\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed for reproducible results\n",
        "# This ensures we get the same 'random' numbers every time we run the code\n",
        "# Very important for learning and comparing results!\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data for our classification problem\n",
        "# Same as before, but now we'll organize it for train/test splitting\n",
        "\n",
        "# Generate dog data (shorter whiskers, higher ear flappiness)\n",
        "dog_whisker_length = np.random.normal(5, 1, 10)      # mean=5, std=1, 10 dogs\n",
        "dog_ear_flappiness_index = np.random.normal(8, 1, 10) # mean=8, std=1, 10 dogs\n",
        "\n",
        "# Generate cat data (longer whiskers, lower ear flappiness)\n",
        "cat_whisker_length = np.random.normal(8, 1, 10)      # mean=8, std=1, 10 cats\n",
        "cat_ear_flappiness_index = np.random.normal(5, 1, 10) # mean=5, std=1, 10 cats\n",
        "\n",
        "# Combine all data into one big array\n",
        "# Each row will be one animal: [whisker_length, ear_flappiness]\n",
        "X = np.vstack([\n",
        "    np.column_stack([dog_whisker_length, dog_ear_flappiness_index]),  # All dog data\n",
        "    np.column_stack([cat_whisker_length, cat_ear_flappiness_index])   # All cat data\n",
        "])\n",
        "\n",
        "# Create labels (0 for dogs, 1 for cats)\n",
        "# This tells us which animal each row represents\n",
        "y = np.hstack([np.zeros(10), np.ones(10)])  # 10 zeros (dogs) + 10 ones (cats)\n",
        "\n",
        "print(\"Total data shape:\", X.shape)  # Should be (20, 2) = 20 animals, 2 features each\n",
        "print(\"Labels shape:\", y.shape)      # Should be (20,) = 20 labels\n",
        "print(\"Labels:\", y)                  # Should show: [0 0 0... 1 1 1...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "# This is the KEY difference from our first notebook!\n",
        "# We're holding back some data to test our final model\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,                    # Our data and labels\n",
        "    test_size=0.2,          # Use 20% for testing, 80% for training\n",
        "    random_state=42         # For reproducible splits\n",
        ")\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)  # Should be (16, 2) = 16 animals for training\n",
        "print(\"Testing data shape:\", X_test.shape)    # Should be (4, 2) = 4 animals for testing\n",
        "print(\"Training labels:\", y_train)            # Labels for training animals\n",
        "print(\"Testing labels:\", y_test)              # Labels for testing animals\n",
        "print(\"\\n📊 We'll train on\", len(X_train), \"animals and test on\", len(X_test), \"animals\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate training data by class\n",
        "train_dogs = X_train[y_train == 0]\n",
        "train_cats = X_train[y_train == 1]\n",
        "\n",
        "# Separate test data by class\n",
        "test_dogs = X_test[y_test == 0]\n",
        "test_cats = X_test[y_test == 1]\n",
        "\n",
        "print(f\"Training dogs: {train_dogs.shape[0]}, Training cats: {train_cats.shape[0]}\")\n",
        "print(f\"Test dogs: {test_dogs.shape[0]}, Test cats: {test_cats.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training and test data\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Training data\n",
        "plt.scatter(train_dogs[:, 0], train_dogs[:, 1], \n",
        "           label='Dogs (Train)', alpha=0.7, s=100, marker='o')\n",
        "plt.scatter(train_cats[:, 0], train_cats[:, 1], \n",
        "           label='Cats (Train)', alpha=0.7, s=100, marker='o')\n",
        "\n",
        "# Test data\n",
        "plt.scatter(test_dogs[:, 0], test_dogs[:, 1], \n",
        "           label='Dogs (Test)', alpha=0.9, s=150, marker='s', edgecolors='black')\n",
        "plt.scatter(test_cats[:, 0], test_cats[:, 1], \n",
        "           label='Cats (Test)', alpha=0.9, s=150, marker='s', edgecolors='black')\n",
        "\n",
        "plt.xlabel('Whisker Length')\n",
        "plt.ylabel('Ear Flappiness Index')\n",
        "plt.title('Training and Test Data Split')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algorithm Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_error(theta, theta_0, dog_data, cat_data):\n",
        "    \"\"\"\n",
        "    Compute training error for given hypothesis parameters.\n",
        "    \"\"\"\n",
        "    error = 0\n",
        "    \n",
        "    # Check errors for dog data (should predict +1)\n",
        "    for i in range(dog_data.shape[0]):\n",
        "        if np.dot(theta, dog_data[i]) + theta_0 < 0:\n",
        "            error += 1\n",
        "    \n",
        "    # Check errors for cat data (should predict -1)\n",
        "    for i in range(cat_data.shape[0]):\n",
        "        if np.dot(theta, cat_data[i]) + theta_0 > 0:\n",
        "            error += 1\n",
        "            \n",
        "    return error\n",
        "\n",
        "def random_linear_classifier(dog_data, cat_data, k, d):\n",
        "    \"\"\"\n",
        "    Random Linear Classifier Algorithm\n",
        "    \"\"\"\n",
        "    best_error = float('inf')\n",
        "    best_theta = None\n",
        "    best_theta_0 = None\n",
        "    \n",
        "    for i in range(k):\n",
        "        theta = np.random.normal(0, 1, d)\n",
        "        theta_0 = np.random.normal(0, 1)\n",
        "        \n",
        "        current_error = compute_error(theta, theta_0, dog_data, cat_data)\n",
        "        \n",
        "        if current_error < best_error:\n",
        "            best_error = current_error\n",
        "            best_theta = theta.copy()\n",
        "            best_theta_0 = theta_0\n",
        "    \n",
        "    return best_theta, best_theta_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on training data only\n",
        "K = 100\n",
        "D = 2\n",
        "\n",
        "best_theta, best_theta_0 = random_linear_classifier(train_dogs, train_cats, K, D)\n",
        "\n",
        "print(\"Best theta (θ₁, θ₂):\", best_theta)\n",
        "print(\"Best theta_0 (θ₀):\", best_theta_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate training error\n",
        "training_error = compute_error(best_theta, best_theta_0, train_dogs, train_cats)\n",
        "training_accuracy = (len(train_dogs) + len(train_cats) - training_error) / (len(train_dogs) + len(train_cats)) * 100\n",
        "\n",
        "print(f\"Training Error: {training_error}\")\n",
        "print(f\"Training Accuracy: {training_accuracy:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate test error\n",
        "test_error = compute_error(best_theta, best_theta_0, test_dogs, test_cats)\n",
        "test_accuracy = (len(test_dogs) + len(test_cats) - test_error) / (len(test_dogs) + len(test_cats)) * 100\n",
        "\n",
        "print(f\"Test Error: {test_error}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot results with decision boundary\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Training data\n",
        "plt.scatter(train_dogs[:, 0], train_dogs[:, 1], \n",
        "           label='Dogs (Train)', alpha=0.7, s=100, marker='o')\n",
        "plt.scatter(train_cats[:, 0], train_cats[:, 1], \n",
        "           label='Cats (Train)', alpha=0.7, s=100, marker='o')\n",
        "\n",
        "# Test data\n",
        "plt.scatter(test_dogs[:, 0], test_dogs[:, 1], \n",
        "           label='Dogs (Test)', alpha=0.9, s=150, marker='s', edgecolors='black')\n",
        "plt.scatter(test_cats[:, 0], test_cats[:, 1], \n",
        "           label='Cats (Test)', alpha=0.9, s=150, marker='s', edgecolors='black')\n",
        "\n",
        "# Decision boundary\n",
        "x_min, x_max = plt.xlim()\n",
        "x_line = np.linspace(x_min, x_max, 100)\n",
        "y_line = -(best_theta[0] * x_line + best_theta_0) / best_theta[1]\n",
        "\n",
        "plt.plot(x_line, y_line, 'r-', linewidth=3, label='Decision Boundary')\n",
        "\n",
        "plt.xlabel('Whisker Length')\n",
        "plt.ylabel('Ear Flappiness Index')\n",
        "plt.title('Random Linear Classifier with Train/Test Split')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Decision boundary: {best_theta[0]:.3f}x₁ + {best_theta[1]:.3f}x₂ + {best_theta_0:.3f} = 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Predictions Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test data\n",
        "def predict(theta, theta_0, data_point):\n",
        "    \"\"\"Predict class for a single data point\"\"\"\n",
        "    return 1 if np.dot(theta, data_point) + theta_0 > 0 else -1\n",
        "\n",
        "# Get predictions for test data\n",
        "test_predictions_dogs = [predict(best_theta, best_theta_0, point) for point in test_dogs]\n",
        "test_predictions_cats = [predict(best_theta, best_theta_0, point) for point in test_cats]\n",
        "\n",
        "print(\"Test Dogs - Actual: +1, Predicted:\", test_predictions_dogs)\n",
        "print(\"Test Cats - Actual: -1, Predicted:\", test_predictions_cats)\n",
        "\n",
        "# Visualize predictions vs actual\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Actual test data\n",
        "plt.scatter(test_dogs[:, 0], test_dogs[:, 1], \n",
        "           label='Actual Dogs', s=200, marker='o', alpha=0.6, color='blue')\n",
        "plt.scatter(test_cats[:, 0], test_cats[:, 1], \n",
        "           label='Actual Cats', s=200, marker='o', alpha=0.6, color='orange')\n",
        "\n",
        "# Predicted classes (using X marks)\n",
        "for i, point in enumerate(test_dogs):\n",
        "    color = 'blue' if test_predictions_dogs[i] == 1 else 'red'\n",
        "    plt.scatter(point[0], point[1], marker='x', s=300, color=color, linewidth=3)\n",
        "\n",
        "for i, point in enumerate(test_cats):\n",
        "    color = 'orange' if test_predictions_cats[i] == -1 else 'red'\n",
        "    plt.scatter(point[0], point[1], marker='x', s=300, color=color, linewidth=3)\n",
        "\n",
        "plt.xlabel('Whisker Length')\n",
        "plt.ylabel('Ear Flappiness Index')\n",
        "plt.title('Test Data: Actual vs Predicted\\n(Circles = Actual, X = Predicted)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNote: Blue X = Predicted Dog, Orange X = Predicted Cat, Red X = Wrong Prediction\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
