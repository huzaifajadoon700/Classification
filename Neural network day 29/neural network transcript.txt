
Search in video
Introduction
0:00
[Music]
0:05
hello everyone welcome to this machine learning teach by doing lecture in this
0:10
lecture we are going to be looking at a Hands-On
0:16
implementation of neural networks in Python we'll be taking a problem and
0:22
then we will construct the neural network architecture we will train the neural network and then we will obtain
0:27
results and then I'll also show you testing we'll be using some standard
0:33
libraries in Python which are routinely used by engineers in practice if you want to see a much more
0:40
detailed version of how to create a neural network from scratch I have started another series
0:46
called neural networks from scratch and that has three to four videos so far in
0:52
which we'll be breaking down the commands which these libraries such as tensorflow and kasas use but in this
0:59
lecture I'm going to use the commands straight from these libraries because
1:04
the main aim of this lecture is to show you how neural networks are used in practice today if you want to get a much
1:11
more theoretical understanding please uh take a look at the neural network from
1:16
scratch series and I'm attaching the link to it in this video for your
1:22
reference so let's get started with today's lecture before we move to the problem and uh before we actually start
1:29
coding in Python I want to First describe about three Frameworks which
Tensorflow, Keras and PyTorch
1:35
typically you must have heard about or you will hear about when you think of coding neural networks in
1:42
Python and uh those three Frameworks are as follows the first is something called
1:47
tensor flow the second is something called kasas and the third is something called
1:56
pytorch now most of the students either use one of these or many students actually don't know the difference
2:03
between these three so let me explain to you each of them first is
2:08
tensorflow and if you go to the tensor flows website you will see that tensorflow is basically a platform for
2:15
running machine learning models or machine learning Frameworks it's developed by
2:20
Google and uh many python codes which are used today use tensor
2:26
flow that's number one the second thing or the second term which you will hear
2:33
about a lot is something called kasas now kasas is a bit different than
2:38
tensor flow because kasas is like an API so basically you can think of it as
2:46
uh basically functions which you can invoke so for example if you want to chain different layers together to
2:53
create a neural network architecture you can use the sequential API call from
3:00
kasas similarly kasas just makes it easier to write down functions if you
3:06
want to implement a Dropout layer instead of coding the Dropout layer from scratch you just have one command for it
3:13
instead of specifying the loss function and instead of explicitly writing down
3:19
the gradient descent rules you can do it very simply using the kasas interface
3:24
which is also called as an API so kasas is a neural network API and
3:30
tensor flow is a platform for ML so what people did was that they integrated kasas with tensor
3:38
flow so tensor flow and kasas are now integrated with each other um and the
3:45
code which we are going to use today we'll actually use kasas within tensor flow so you can think of as tensor flow
3:52
as the bigger machine learning library kasas as the neural network API so
3:57
basically making it easier to call neural network functions and then kasas was merged with tensor flow so that you
4:04
we can call the kasas functions by using tensor flow itself then there is one more Library
4:12
which is called as P torch now P torch is also a machine
4:17
learning framework and you can Define and run neural networks using python using pytorch also it was developed by
4:24
meta which was earlier called Facebook now there is a big difference
4:30
between P torch and tensor flow and I will try to simplify that difference for
4:35
you as much as possible so uh first let's look at simple codes in tensor
4:42
flow and py torch which you will write to create a neural network so for example
4:48
U here we are using tensor flow you import tensor flow as TF and then
4:53
remember I told you kasas is uh integrated with tensor flow so if you
4:59
want to use the layers API within kasas all you need to do is from tensorflow
5:04
docas import layers and then you can Define the layers of the neural network
5:09
you can even import sequential so here we have defined a neural network which has 10 input features and one
5:17
output on the other hand if we wanted to do do the same thing with pytorch the
5:22
implementation kind of looks like this so pytorch follows a more objectoriented
5:27
approach where we define a class then we Define two methods within the class so underscore underscore init
5:34
underscore underscore is a special method which is called automatically when an instance of this class is
5:40
created then we uh Define the weights of this neural network 10 input features
5:46
one output the second method is forward so this implements a forward pass and
5:53
then Returns the output from the neural network so there are two different ways
5:59
of of writing the same neural network tensorflow uses more like an API based
6:04
approach or chaining functions together and P torch uses uh an objectoriented
6:12
programming approach but there is a subtler and a bigger difference between these two tensor flow uses something
6:19
which is called as a static computation graph and pytor uses something which is called as Dynamic computational
6:27
graph so let me try to actually explain to you what these
6:32
mean okay so let's say we want to compute the summation of two functions
6:37
so remember tensor flow uses static computation graph so we Define the compute function and then we say that
6:44
Cal to A+ B now if you want to modify something in this function you need to
6:51
write a different function so compute extended Cal to a + b and then let's say
6:56
I want to Define another variable called D which is C multiplied by 2 so that is
7:02
another variable so you cannot just extend this function very easily you
7:08
have to write another function the reason is because once this function is defined a static computation graph is
7:14
created by tensor flow and it becomes very hard to be flexible with this function now that's the simplest way to
7:21
describe it whereas if you use P torch you can change things on the fly so in
7:29
initial addition can be performed dynamically where C = to a + b then you can also add another operation
7:35
dynamically d = c into 2 you don't have to define a separate function for it
7:40
essentially what I'm trying to say is that P torch is a bit flexible with the code because it creates computational
7:46
graph between variables on the fly or on the go as the code is being run whereas
7:53
uh tensor flow creates the graph Dynamic uh creates the graph in a static manner
7:58
so if you want to change something then we need to redefine the function itself
8:04
and uh so it's a bit less flexible in summary both tensor flow and P torch can be used by beginners uh but
8:12
remember that tensor flow is fast robust but it's not very flexible so if you want to toy around
8:19
with stuff if you want to use this for research tensor flow is not very recommended but it's actually quite good
8:26
for production level or industry level machine learning framework Works P torch on the other hand generates these graphs
8:33
in a dynamic manner so it's very flexible so if you want to change stuff on your own be use these Frameworks to
8:41
do research then P torch is very good as a beginner you can use both pie
8:47
torch and tensor flow and in fact I encourage you to use both to see the similarities and the differences between
8:54
the two in this particular uh video we are going to be using tensor flow
9:00
so uh I hope you have understood the differences between these three Frameworks now tensor flow caras and P
9:07
torch basically you can think of tensor fluent kasas in one bucket and pch in another
9:13
bucket so let's get started with today's example today what we are going to do is
Problem introduction
9:18
we are going to perform a simple classification task first let me introduce you to the input data our
9:25
input data or training data looks something like this every input has two
9:30
attributes X1 and X2 so if you plot them it will look like this and every input
9:36
also has a color coding associated with it so essentially there are three color codes r g or B so any point X1 comma X2
9:47
which you take let's say we take this point this is coded as red if we take this point it's coded as blue if we take
9:53
this point it's coded as green this is also called as the spiral data our goal in this lecture will be to
10:00
design a neural network uh which takes in this which
10:05
takes in an input and can make a prediction regarding whether it belongs to the r category G category or B
10:11
category based on the training data I hope I have made the problem
10:16
clear if any new input is given to you let's say which lies somewhere here or even if the input is in the test
10:24
data and not in the training data the neural network should take the input and classify
10:29
which of the three color categories the input belongs to so this is clearly a classification
10:35
problem um so to start thinking about this problem itself first you need to
Neural network architecture choice
10:41
think of the neural network architecture and this is a bit flexible so you can start out with anything which you want
10:48
the architecture which I'm going to start out with is this I'm going to have 64 units in the first layer and I'm going
10:56
to have 32 units in the second layer so I'm going to have input layer in which X1 and X2 come in then I'm going to have
11:03
first hidden layer second hidden layer and then the output layer remember output layer should have three neurons
11:09
because we want r g and B okay so the first hidden layer has 64 units so let
11:17
me Mark these with a tick first hidden layer has 64 units second hidden layer
11:23
has 32 units okay another Choice which you need to make apart from this is what
11:29
are the activation functions which you need to use so the First Choice which we made is number of hidden layers second
11:36
choice which we made is number of neurons third choice is the activation layer in each layer or the activation
11:43
function in each layer so we need an activation function for the first hidden layer one activation function for the
11:49
second hidden layer and we need one activation function for the output layer so uh we have already covered a
11:59
lecture on activation functions so remember that for classification tasks the output layer has to have a softmax
12:05
Activation so this is fixed because there are not too many options so if the input belongs to the
12:13
red class the first neuron will have the highest probability second and third will have low probabilities or they will
12:19
not fire we only want the first neuron to fire if the input is red so the output neuron should essentially
12:25
represent probabilities and that's why we will be using the soft Max activ in the output layer now what is to be used
12:32
in the hidden layers is generally flexible and you can use many activation
12:37
functions such as reu sigmoid T Etc I'm going to be using reu because it's the
12:44
safe initial choice to go with so if you are a beginner I would suggest to use reu in the hidden layers and if it's a
12:52
classification task you can use softmax in the final layer if it's a regression task you can always use reu in the final
12:58
layer layer okay so this is the architecture which we are going to use and I have
13:05
just represented it over here the second choice which we need to
Optimizer and loss choice
13:11
make is regarding the optimizer and regarding the loss so once we fix the output activation the loss also kind of
13:18
gets fixed because we using the soft Max Activation so the loss which comes with
13:23
the softmax is the categorical cross entropy loss just to uh revise your
13:30
memory the categorical cross entropy loss looks something like this negative Sigma true label into log of
13:40
predicted so uh another simple way to think about this is just negative log of
13:46
the correct class prediction so if the input correct class is red we look at
13:51
the three outputs and we if let's say the outputs are 7 2 and 0.1 and the
13:59
input is R its true value is R which means red category so the other two
14:06
neurons don't matter to us because they don't belong to the correct class so these two don't matter so the loss will be negative log
14:13
of 7 which will be around I think 35 so as
14:20
this output gets closer and closer to one the LW starts decreasing and when this output becomes equal to one of the
14:27
first neuron then the loss become zero because then the that's exactly what we needed right since it belongs to the red
14:35
category so this is the categorical cross entropy loss which we'll be using and for the optimizer we'll be using the
14:42
Adam Optimizer which is generally the most commonly used Optimizer in neural
14:47
network tasks like these I'll also show you uh stochastic gradient descent and
14:54
stochastic gradient descent with momentum we have had lectures for these in the past P so you will directly
15:00
relate to what we learned in the theoretical lectures so I hope you have uh
15:06
understood the architecture which we are going to go ahead with and the optimizer choices which we are going to go ahead
15:13
with remember that in the future if you want to solve any problem using neural networks before jumping into code it's
15:20
always better to make a rough sketch think of the units the activation the
15:25
output activation then the loss function and the optimizers once you have thought this
15:31
through then it's a good idea to move towards the code so now I'm going to be
15:37
uh taking you to the coding interface in vs
15:42
code so first thing which I'm going to do here is zoom in a bit so let me go to appearance and zoom
Python code: Training
15:50
in and let me actually zoom in one more time just so that U we all can see the
15:56
screen clearly so appearance zoom in one more time
16:02
great so I think this should be
16:10
enough okay so this is the code and I've have named it spiral nn.
16:16
iynb uh I'll be sharing this code with you so don't worry okay so the initial
16:22
task is to load the packages so I'm going to comment here loading
16:31
the packages and we are going to load the following package we are going to load
16:37
numpy because it will help uh creating arrays for scientific Computing it's always preferred I'll use matte plot lib
16:45
essentially for plotting then um I'm going to be using SK learn which is s kit learn because I
16:52
want to use the train test split to split the data into training and testing then I will use uh
16:59
again Sait learn and import standard scaler this helps to take subtract the
17:05
mean and normalize the data which we'll be doing for both the training and testing data and finally as I mentioned
17:12
to you before we'll be using kasas so we'll be using two uh functions from kasas sequential
17:20
and dense sequential is to chain the different layers
17:26
together and dense is essenti to create a fully connected dense neural network
17:32
so let's get started the first task which we'll be doing is to generate the data itself so I'm not going to be
17:40
focusing on how I have generated the data here because you can generally for all neural network related problems the
17:46
data is already available for the sake of demonstration I have just written some function using S and cosine to be
17:54
uh so that we can generate the spiral data so I'm going to run this part part of the code now where we'll be also
18:00
plotting the spiral data I'm taking a total of 999 points 333 will belong to
18:08
the red class 333 will belong to the green class and 333 will belong to the
18:13
blue class so let me run this okay so I have run this piece of
18:20
code now and you can see that this is how the uh spiral data actually looks
18:27
like uh I had already shown this to you in the Whiteboard but here you can see we have generated this in vs code the
18:33
input has two attributes X1 and X2 and each data point has a color either it's
18:39
red it's green or either it's red it's green or it's blue great so the data has been
18:46
generated now uh after the data has been generated it is very important to have
18:52
uh training data and testing data because ultimately what is important to
18:58
us is how well we are doing on the testing data so we are going to use this command called train test split which is
19:06
obtained from s kit learn. model selection what this command does is that
19:11
once we give the X data and the Y data it will automatically uh divide it into
19:18
training and testing based on the test size so here I'm giving a test size of
19:23
02 which means we'll use 20% for testing and we'll use 80% for training let me
19:29
just quickly plot the X and Y values for you to see if you plot the X
19:37
values they will look something like this these are X X1 comma X2 pairs uh and if you plot the Y value
19:45
they will look something like this Zer or ones or two so that's r g or
19:52
B I'm going to delete this cell now so these are the uh X so now this is the
19:59
train test split X train and X test will be the X X1 X2 values for the training
20:04
and testing and Y train and Y test will be y1 Y2 values uh for training and
20:14
testing or rather not y1 Y2 Valu 0 1 or two for training and the testing
20:21
data and remember this random State helps us to initialize the same way every single time
20:29
uh so this is the command which you need to do to split the data into training
20:34
and testing when I share this code with you I encourage you to try different test sizes and see the output and how it
20:40
changes ideally if the test size is too large the error will be very high but if
20:47
the test size is very very small we are likely to be overfitting so that's also not good usually I start with 0
20:54
2 then we are going to use the standard scalar function and we are going to uh
21:00
apply this to the X strain and we are also going to apply to
21:06
the uh X test the reason is that uh we want to
21:13
basically uh standardize the data essentially what we want is
21:20
that let's say if you have uh data and
21:26
let's say one data X1 let's say is very high and X2 or
21:33
let's say one input value is very high and second input value is very low what
21:39
this actually does is that it confuses the neural network and the training time increases that much so what we do is
21:46
that it's a common practice to take the input values so let's say this is the X1 X2 input values right uh X1 X2 1 and we
21:56
have 999 values like this so what is done is that from this Matrix
22:02
we first subtract the mean uh so mean of
22:08
X and then we divide the standard deviation of
22:13
X so what this will do is that every single row will kind of have similar
22:18
minimum and maximum values so it will never happen that one
22:24
value one feature value is incredibly large while other feature value is
22:30
incredibly small this will never happen we will make sure that everything is standardized and that's the reason why
22:37
uh this scaler do transform is used for the training data and for the testing
22:43
data it's not completely it's not needed every single time but I have seen that
22:49
using the scaler do transform it accelerates the training process because
22:54
then features are uh of the similar magnitude and similar
23:01
scale okay so up till now we have the training data we have the testing data
23:06
so let us actually visualize the xtrend scaled and let us see what it looks like
23:11
so if I just let me scroll up a bit so here I'm
23:19
visualizing X train scale so look at these values versus if you just look at X Trin
23:29
uh and uh xra scaled you can see that the values are
23:37
first of all different they are not the same um and this is because we have used this uh subtraction of the mean and
23:44
divide by the standard deviation so here you can see that all values kind of lie
23:49
between minus one and one there are some values which go up to min-2 let's say but almost all the values will have
23:55
similar input similar minimum and maximum ma imum values next what we'll do is that we
24:02
will actually Define the neural network architecture exactly like what we had seen over here so remember what we saw
24:10
here we are going to use two hidden layers the first layer has 64 units and
24:17
the second layer has 32 units and Ru reu and the final one has soft Max okay so
24:24
let us Define the model first I do this sequential and this is from kasas this
24:30
tells kasas that we are defining the neural network and we want to chain different things together so first we
24:37
create a dense layer the reason we use dense is because it's a fully connected layer and the first layer has 64 units
24:45
and it has two inputs and the activation is Ru so look at the first layer the first
24:52
layer has 64 units it has two inputs and the activation is ru
24:58
this is exactly what has been defined over here then what we actually do is we add
25:04
another layer whose uh which is fully connected which has 32 inputs and whose
25:11
activation is Ru we don't need to define the input Dimension here because it's automatically understood that the output
25:18
of this first layer is feeding into this so we don't need to specifically write the input Dimension again and then we
25:25
write the final layer so remember the final layer layer essentially has three
25:30
neurons and the soft Max so when we write the final layer it will look something like it has three neurons and
25:37
the activation is soft Max great so I'm just running this part of the code and right now it's
25:43
done and now we come to one of the last steps of the process which is writing the optimizer so the neural network
25:49
architecture has been defined uh weights have been initialized but weights have not been optimized yet and for that we
25:56
need to define the optimizer as we saw here we are going to be using the Adam Optimizer and later I'm going
26:04
to show stochastic gradient descent and stochastic gradient descent with momentum so first let us say that
26:10
Optimizer is ADM and the loss is categorical cross entropy so the loss
26:17
which is to be used is the categorical cross entropy loss and the metric is accuracy which means we'll be looking at
26:25
accuracy uh when we actually compare the model performance so let me run this so
26:32
remember this comes under model. compile so up till now uh from kasas we have
26:38
used model. add and model. compile and the last thing which we'll be using from
26:44
kasas is model.fit what this will actually do is that after the after you
26:50
have defined the optimizer and after we have defined the loss we'll be doing the training process which means that the
26:56
weights of the neural network will will be adapted and ultimately they will be
27:02
adapted so that the loss between the data training data and our neural
27:08
network prediction on the training data is as low as possible um and here I'm also plotting
27:15
the validation data which means how well we are doing on the testing set I'm doing 50 EPO so let me run this
27:22
particular code so I'm running this right now here you can see the EPO are updating 1 out of 502 out of 50 and you
27:29
can see the accuracy at every step uh so the accuracy started out with 41 which
27:36
was not very good but here you can see as we reached 50 apox the accuracy reached 9946 which is very good remember
27:45
epox is going through the entire training data once so we are doing this 50 times and the validation loss it's
27:52
important to look at the validation loss also because usually that is the metric which we actually care about how well we
27:58
are doing on data we have not seen so validation loss was also pretty high initially but as we go down you can see
28:05
that the validation loss went to 0.0 225 which is very
28:11
good now how do we know whether the validation loss is enough or not or should we keep on going further one good
28:18
way is to plot the training loss to plot the validation loss to plot the training
28:24
accuracy and to plot the validation accuracy so let me plot this
28:31
actually so here I have plotted this four and I want you to very keenly look at first let's look at the training loss
28:38
that's written by this blue arrow or this blue line the reason we know 50 EPO
28:43
are enough is that the training loss has started to decrease and then it has started to stagnate now so probably it
28:49
won't go even further then but we will test this just in a moment the validation loss is also
28:56
decreasing which is very good the training accuracy which is this green line is increasing and it has stagnated
29:02
so you see for 30 40 50 EPO the training accuracy is stagnated and the validation
29:08
accuracy has also stagnated this means that we probably don't need too many AO
29:13
more but let us see what happens if we increase the APO so now I'm going to increase the APO to 200 just keep this
29:20
final accuracy score in mind 9946 and let us see how we are doing with 200 EPO so I'm running this right
29:27
now and here you can see this will run for a bit longer period of time because
29:33
now we are running it for 200 AO so 50 EPO are completed and it's still
29:40
running now around 90 EPO are completed and U now we are at around
29:46
120o so it's a it's fast so we don't need to wait for too much amount of time
29:52
but already if you see at the loss it's not changing too much after this point uh which which was kind of what we
29:59
expected so now 200 EPO have been done so let me plot this so this is the plot with 200 aox
30:06
you can see that after 50 everything is stagnated even the accuracy has
30:12
stagnated so we don't need these many APO it's it's fine to have 50 APO as
30:17
well so this is how you actually judge how many AO you need you look at the
30:23
plot of the training loss training accuracy and you look at the plot of the validation loss and the validation
30:31
accuracy uh if the plots actually start to stagnate so I'll need to initialize once more to run
30:40
this so if the plots actually start to stagnate it is a sign that we don't need more EPO we can stop at that particular
30:51
Point okay so here it's again showing the plots for 50x the loss plots and the
30:57
validation plots the loss plots and the accuracy plots have stagnated for the training and validation data so which
31:03
means the architecture chosen is pretty good and we have reached an amazing level of accuracy for the test data this
31:11
is how you train neural network models using kasas it's very simple you see we just did three things using model. add
31:18
we created the architecture using model. compile we specified the optimizer and
31:23
we specified the loss and using model. fit we actually did the training so all the lectures about back
31:30
propagation which we looked at in the previous lectures those are actually covered in this fit command so back
31:37
propagation is happening underneath the surface or underneath the hood as it's called when we run this command if you
31:44
want to see how back propagation actually works in action I encourage you again to follow the neural networks from
31:49
scratch series okay great so we have uh the loss
31:54
plots and we have the accuracy plots but now let's do an important thing let's actually compare the true value with the
Python code: Testing
32:02
predicted value so in this particular piece of code what I'm doing is that I'm
32:07
actually just writing the predictions for the test data and uh I'm plotting the
32:15
predictions so here you can see here you can see in one figure on
32:22
the left hand side you can see the true classes on the right hand side you can see the predicted classes and you can
32:28
see that our neural network is predicting perfectly and remember this is test data so this was Data not seen
32:35
to the neural network this is not the training data of course it will do well on the training data because it has seen
32:41
that data but this is the validation data and we already knew this would be good because look at the validation loss
32:47
the validation loss has become almost zero and the validation accuracy is almost 90% as we have
32:55
seen so that's why the true classes and the predicted classes for the test data look almost exactly alike this is a
33:03
great because our neural network is not overfitted it learned very well on the test
33:08
data I want to do one more thing now I want to show you if a new point is given what is the classification of our neural
33:15
network so remember that once the neural network is trained for any point given to it it will predict whether it's r g
33:21
or B so if your question is that what if a point is somewhere not in the test data but somewhere else it's fine our
33:28
neural network is now trained so it will predict for that point so I'm going to show you those predictions so I'm going
33:35
to take a new Point as 0.1 and minus5 so it will lie somewhere over here 0.1 and
33:42
minus5 so it will lie somewhere over here and I want to predict whether it belongs to the red the green or the blue
33:50
so it kind of Lies closer to the blue right so I hope the neural network gives blue as the answer but let's see so
33:57
again I'm doing the prediction here uh so I'm doing the prediction based on my
34:02
final predict so remember if you want to do the prediction you need to use the model. predict from
34:09
kasas so I'm doing the prediction and I'm encoding that if the prediction is zero uh use the legend as red if it's
34:17
one show me green and if it's two show me blue so let me run this so look at
34:22
this particular so this is that data this is the new data point which is
34:27
shown in yellow and look at the prediction the neural network is predicting that the new prediction new
34:33
point is blue this is exactly what we had hoped right because it's more closer to the blue data remember that this is
34:40
not K nearest neighbor so although our intuition says that this yellow is closer to Blue this data point is closer
34:47
to Blue so it should be classified as blue blue it does not work like that since it is not K nearest neighbors but
34:53
I was just telling you for visualization purposes so So within just 60 to 70
34:59
lines of code I have demonstrated how you can run code and run a simple neural network for a spiral data like this and
35:07
how you can classify which class it belongs to red green or blue now think
35:13
of all the types of problems you can solve using the same code you can solve any kind of classification problem for
35:20
regression it's exactly similar you just have to change the activation function in the output to maybe a real BL
35:27
activation function so you can use this code as a template to solve many problems with neural networks one thing
35:35
I wanted to show you is the loss when we use different optimizers so now if we
35:40
use a stochastic gradient descent Optimizer let's say uh let us see the loss with it remember with Adam the loss
35:47
was going to almost zero and the accuracy was going to around 99 let's see what the accuracy is with
35:54
the stochastic gradient descent so here I'm using tf. k. optimizers SGD which is
36:01
stochastic gradient descent and I'm going to run this uh okay and I will run 50 epox
36:07
again so let us try to have a look at the accuracy so you can see that the
36:13
accuracy is struggling a bit right the maximum accuracy which it reached is
36:19
7799 and if you plot this here here also you see that the accuracy is not
36:24
actually look at the accuracy plots it's not going to .9 this is again an
36:29
indication that ADM is a better Optimizer than stochastic gradient descent for most neural network problems
36:36
now let us try to use momentum uh as we saw momentum really helps accelerate gradient descent and it
36:43
is possible to give momentum as an argument here so I'm now including
36:48
momentum so momentum remember um uses a running average of historical data
36:58
and uh that way it actually helps in Faster training and it can lead to more
37:04
efficient or accurate values so let me run this now first let me reinitialize
37:11
the neural networks let me run this model. compile model.fit and now let me
37:17
observe the accuracy so here you can see as expected the accuracy is now going to
37:22
99 so the accuracy is now 9925 let me show this in plot amazing so now look at
37:29
the accuracy plots they have reached around .99 which is exactly similar to what we had observed for Adam this is
37:36
the advantage of using momentum in fact if you have learned about the Adam Optimizer it just a mix of momentum and
37:43
an Optimizer called Ada Delta so we would kind of expect that momentum does better than stochastic gradient
37:50
descent so like this once you get this code you can play around with different things you can play around with the
37:57
train test split you can play around with removing this normalization you can play around with
38:03
the optimizers the units the layers and you can also play around with momentum
38:08
values different optimizers and the number of epoch as well so remember that
38:14
for training a neural network like this the real skill of an engineer comes from also knowing what is the architecture to
38:21
use what are the units to be used so let me make a list of what all is important
38:27
so first what is important is the architecture what are the number of
38:33
units to be used what are the activation functions to be used second thing which is important is
38:40
Optimizer so which Optimizer do we need to use do we need to use the ADM do we
38:45
need to use stochastic gradient descent do we need to use the momentum if we are using the momentum what should be the
38:53
value for the momentum parameter third thing is what is the
38:58
loss function which we want to use fourth thing is what is the train
39:04
test split fifth thing is that do we want to
39:11
normalize the inputs and there are some other
39:16
considerations which I have not covered here should I add a Dropout layer should I do something called like
39:22
batch normalization so right now we only normalize the input data right we are
39:28
not normalizing the output which comes from each layer so in batch normalization that is also done which
39:33
sometimes improves efficiency Dropout also prevents uh overfitting and it
39:39
leads to regularization which improves the generalization or the test accuracy
39:45
in this example we did not need to do uh these two things these two things
39:51
Dropout and normalization batch normalization but they are quite important so as a neural Network or as a
39:58
machine learning practitioner remember that always keep these six things in mind do not just blindly execute codes
40:06
but always try to think from scratch what are the effects of each of these things and then change them accordingly
40:12
and I hope the previous lectures which I have had on neural networks will help you choose different things like
40:17
architecture Optimizer loss function train test split Etc so thank you so much and uh I hope
40:25
this lecture added value to your life I'll be sharing the code file in the information so that it will help you run
40:31
this code and also run other codes so again thank you everyone and I look forward to seeing you in the next
40:37
lecture
Machine Learning: T