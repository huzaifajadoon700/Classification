{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496c02dc",
   "metadata": {},
   "source": [
    "# Ridge Regression Visualization\n",
    "\n",
    "This notebook visualizes why it's called \"Ridge\" Regression by showing:\n",
    "1. The loss surface for OLS with dependent features\n",
    "2. The \"ridge\" that forms at the bottom of the loss surface\n",
    "3. How regularization lifts this ridge to create a unique minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09df7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67625bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with dependent features\n",
    "X = np.array([[1, 2],\n",
    "              [2, 4],\n",
    "              [3, 6]])  # X2 = 2X1\n",
    "Y = np.array([3, 5, 8])\n",
    "\n",
    "# Define loss functions\n",
    "def ols_loss(beta1, beta2):\n",
    "    \"\"\"Ordinary Least Squares loss function\"\"\"\n",
    "    return np.mean([(Y[i] - (beta1*X[i,0] + beta2*X[i,1]))**2 \n",
    "                    for i in range(len(Y))])\n",
    "\n",
    "def ridge_loss(beta1, beta2, lambda_reg=1.0):\n",
    "    \"\"\"Ridge Regression loss function\"\"\"\n",
    "    return ols_loss(beta1, beta2) + lambda_reg * (beta1**2 + beta2**2)\n",
    "\n",
    "# Create grid of beta values\n",
    "beta1_range = np.linspace(-2, 2, 100)\n",
    "beta2_range = np.linspace(-2, 2, 100)\n",
    "B1, B2 = np.meshgrid(beta1_range, beta2_range)\n",
    "\n",
    "# Calculate loss surfaces\n",
    "Z_ols = np.zeros_like(B1)\n",
    "Z_ridge = np.zeros_like(B1)\n",
    "for i in range(len(beta1_range)):\n",
    "    for j in range(len(beta2_range)):\n",
    "        Z_ols[i,j] = ols_loss(B1[i,j], B2[i,j])\n",
    "        Z_ridge[i,j] = ridge_loss(B1[i,j], B2[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ec90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both loss surfaces\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "# OLS Loss Surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf1 = ax1.plot_surface(B1, B2, Z_ols, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('beta1')\n",
    "ax1.set_ylabel('beta2')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('OLS Loss Surface\\nNote the ridge at the bottom')\n",
    "\n",
    "# Ridge Loss Surface\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "surf2 = ax2.plot_surface(B1, B2, Z_ridge, cmap='viridis', alpha=0.8)\n",
    "ax2.set_xlabel('beta1')\n",
    "ax2.set_ylabel('beta2')\n",
    "ax2.set_zlabel('Loss')\n",
    "ax2.set_title('Ridge Loss Surface\\nNote the unique minimum')\n",
    "\n",
    "# Add colorbars\n",
    "fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
    "fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot contours to better see the ridge\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# OLS contours\n",
    "c1 = ax1.contour(B1, B2, Z_ols, levels=20)\n",
    "ax1.set_xlabel('beta1')\n",
    "ax1.set_ylabel('beta2')\n",
    "ax1.set_title('OLS Loss Contours\\nNote the valley along the ridge')\n",
    "\n",
    "# Ridge contours\n",
    "c2 = ax2.contour(B1, B2, Z_ridge, levels=20)\n",
    "ax2.set_xlabel('beta1')\n",
    "ax2.set_ylabel('beta2')\n",
    "ax2.set_title('Ridge Loss Contours\\nNote the clear minimum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60ff8d",
   "metadata": {},
   "source": [
    "# Understanding the Visualization\n",
    "\n",
    "The plots above demonstrate why this technique is called \"Ridge\" Regression:\n",
    "\n",
    "1. **OLS Loss Surface**:\n",
    "   - When features are dependent (X2 = 2X1), the loss surface has a ridge at the bottom\n",
    "   - This ridge means there are infinite solutions with the same (minimum) loss\n",
    "   - Any point along this valley gives the same prediction quality\n",
    "   - This is problematic because some of these solutions have very large coefficients\n",
    "\n",
    "2. **Ridge Loss Surface**:\n",
    "   - Adding the regularization term (λ(β₁² + β₂²)) \"lifts\" the ridge\n",
    "   - Creates a unique minimum point instead of an infinite valley\n",
    "   - Forces the solution to have smaller coefficients\n",
    "   - Makes the solution more stable and interpretable\n",
    "\n",
    "The ridge in the OLS loss surface is what gives Ridge Regression its name, as the regularization term specifically addresses this ridge by creating a unique minimum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
